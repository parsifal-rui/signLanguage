### 一、 核心背景知识（开工前必须要懂的“行话”）

在动手之前，你需要先在认知上把“手语”、“图像生成”和“数字人”这三个领域祛魅：

**1. 手语语言学：不要字对字翻译，要懂“Gloss（手语词）”**

* **知识点** ：手语有一套独立的三维空间语法。比如中文说“你叫什么名字？”，手语的语序是“你 / 名字 / 什么 / （配上疑问的面部表情）”。
* **行话** ：在计算机领域，这种剥离了语音、仅仅代表手语动作含义的文本序列，被称为  **Gloss** 。你的系统第一步，永远是把自然语言（Text）转化成 Gloss。

**2. 图像生成 vs 3D 骨骼：为什么不要用视频生成（Sora/Runway）做手语？**

* **知识点** ：你可能会想“能不能我输入一句话，让 AI 直接生成一段手语视频？”**绝对不要这么做。** 目前的生成式 AI（哪怕是最前沿的视频大模型）存在严重的“时空不一致性”，最典型的就是“画不好手”（经常出现六根手指或手指融合）。
* **结论** ：做手语教学工具， **必须走 3D 骨骼动画路线** 。它虽然看起来像游戏建模，但每一次手指的弯曲角度都是精确、可控的。

**3. 数字人与动作格式：BVH 与 SMPL-X**

* **知识点** ：数字人其实就是一个“皮囊”（Mesh）加上一副“骨架”（Skeleton/Rig）。
* **行话** ：你需要了解  **BVH** （最通用的 3D 动作捕捉数据格式，记录了每个关节在每一帧的旋转角度）和  **SMPL-X** （学术界最常用的参数化人体模型，能精确控制身体、双手和面部表情）。

---

### 二、 技术实现 Roadmap（MVP 最小可行性产品管线）

要跑通你的产品，我们需要构建一个三段式的漏斗：`Text (文本) -> Gloss (手语词) -> Pose (动作数据) -> Avatar (3D渲染)`。

#### Phase 1: 文本到手语语序转换 (Text-to-Gloss)

* **目标** ：把用户的日常大白话，洗成符合手语逻辑的词汇序列。
* **实现路径** ：
* **最快方案** ：直接调用大语言模型（如 DeepSeek、通义千问 API）。你需要写一套非常强大的 System Prompt，喂给模型中国手语（CSL）的语法规则，让它做 Few-shot prompting（少样本提示）。
* **输入** ：“我明天去北京看病。” ->  **LLM 输出** ：`["明天", "我", "去", "北京", "看病"]`。

进度check

CSL-Daily数据集拿不到，娘的

* 最值得拿来做 Phase 1 可验证数据：CSL-Daily。
* 每条数据 ≈ 一句中文 + 对应 Gloss 序列 + 视频（你 Phase 1 主要用前两者）。
* 拿到后可以：
* 划出一部分做 验证集（看 LLM 输出的 Gloss 和标注的 Gloss 有多接近）
* 从训练集里挑几条做 few-shot 示例（就像上面那样写进 prompt）

娘的 apikey也没有

#### Phase 2: 动作映射与串联 (Gloss-to-Pose)

* **目标** ：建立一个字典，把上面的词汇对应到具体的 3D 动作文件上。
* **实现路径** ：
* 你需要构建一个小型的“动作资产库”。初期不需要自己动捕，可以在网上（如 Mixamo 或开源的动捕数据集）找到对应的 `.bvh` 或 `.fbx` 动作文件。
* 写一段控制逻辑：系统接收到 `["明天", "我", "去"]`，就依次从数据库里把这三个词的动作文件提取出来，排在一个时间线上。

你提到的“你好”不用把手放下的问题，在学术界被称为提取手语的 Stroke（核心表达阶段）。
一个完整的手势轨迹其实分为三个阶段：
1. Preparation（准备）：手从身侧抬起到胸前。
2. Stroke（核心表达）：真正打出“你”这个动作的瞬间。
3. Retraction（收回）：手放回身侧。

google mediapipe:
https://mediapipe-studio.webapps.google.com/studio/demo/hand_landmarker
hand_landmarker就够用了

#### Phase 3: 前端 3D 渲染与融合 (Rendering)

* **目标** ：把排好序的动作喂给一个 3D 模型，让它在网页或 App 里动起来。
* **实现路径** ：
* **Web 端首选** ：使用  **Three.js** （一个极度普及的 WebGL 库）。
* 加载一个免费的 3D 人物模型，将 Phase 2 串联好的骨骼动作数据绑定上去，在浏览器里实时渲染出来。
 
 数据流向大重构：摆脱 BVH 束缚既然你要用 MediaPipe 自己录 100 个词的数据集，你的数据格式将发生根本性变化。旧思路（死胡同）：录视频 -> 尝试转成 BVH 格式 -> 导入 Three.js -> 发现骨骼不匹配 -> 崩溃。新思路（畅通无阻）：Python + MediaPipe 提取你录制的视频，拿到每一帧手部和身体的 3D 坐标 $(X, Y, Z)$。Python 向量数学 将这些坐标转化为每个关节的局部旋转角度（四元数 Quaternion）。把处理好的、剥离了多余动作的纯四元数数据，存成一个极度轻量的 JSON 数组。前端 Three.js 只做一件事：读取 JSON 里的四元数，逐帧赋值给 VRM 模型的骨骼（vrm.humanoid.getNormalizedBoneNode('...').quaternion.fromArray(...)）

 Task 1: MediaPipe 数据采集器 (Python)

Prompt 思路：“写一个 Python 脚本，使用 OpenCV 读取本地视频，利用 MediaPipe Holistic 提取身体 33 个关键点和双手 42 个关键点的 3D 坐标。计算相邻帧手腕关节的速度，绘制速度曲线，帮我找出动作停顿的 Stroke 阶段，并将这几帧的坐标导出为 JSON。”

Task 2: 坐标到旋转的数学转换 (Python)

Prompt 思路：“我有一组人体骨骼的 3D 绝对坐标点。请帮我写一个 Python 函数，计算出肩关节、肘关节、腕关节对应的相对旋转四元数（Quaternion）。请参考 VRM Humanoid 的 T-Pose 骨架标准，最后输出格式为每个关节包含 [x, y, z, w] 的 JSON。” (注：这是最需要数学的一步，但在 Cursor 里，你只要描述清楚父子节点的向量关系，AI 能直接把数学公式写出来。)

Task 3: 前端 VRM 渲染器 (HTML/JS)

Prompt 思路：“使用 Three.js 和 @pixiv/three-vrm 库，在网页中加载一个本地的 .vrm 模型。写一个动画循环，读取我提供的 JSON 文件（包含每一帧各骨骼的 quaternion），利用 slerp 方法在帧与帧之间做平滑插值，驱动模型实时做出动作。”
---

### 三、 你一定会撞上的技术壁垒（Boss战）

在做这个极其性感的人机交互产品时，有几个硬骨头你需要提前有心理准备：

**1. 协同发音（Co-articulation）与动作融合（最致命壁垒）**

* **痛点** ：如果你只是把“我”和“去”的视频硬拼接在一起，数字人会在做完“我”之后，手臂瞬间闪现（瞬移）到“去”的初始位置，像抽搐一样，极其违和。
* **解法** ：需要在两个动作切换的几帧里，做 **运动学插值（Cross-fade blending）** 。但简单的线性插值经常会导致“穿模”（比如手从身体里穿过去）。如何让过渡动作既平滑又符合人体物理学，是前端渲染的难点。

**2. 极度匮乏的“动作标注数据集”**

* **痛点** ：英文手语（ASL）有很多开源的高精度 3D 动作集（比如 How2Sign），但中文手语（CSL）高质量的 3D 骨骼动作数据极少。
* **解法** ：如果你找不到现成的动作文件，可能需要用 **Google MediaPipe** 去跑 YouTube 或 B 站上的手语教学视频，把视频里老师的 2D 动作强行提取成 3D 骨骼坐标（Pose Estimation），自己攒一个小型数据库。

**3. 非手控特征（面部表情）的缺失**

* **痛点** ：手语的灵魂在于表情。普通的 3D 骨骼往往只有身体和手，面部是僵硬的（死鱼眼）。
* **解法** ：后期需要引入 Blendshapes（面部形态键）技术，在翻译 Gloss 的同时，给模型下发表情指令（例如：打出“疑问”动作时，触发“皱眉”的 Blendshape 参数）。

---

我发现的问题

1. 手语的连读：比如“你好”，两个动作都要伸出手，所以不是简单的你和好之间的拼接，省去了“你”结束后把手放回的动作
2. 具体来说有了骨骼怎么渲染人体呢
